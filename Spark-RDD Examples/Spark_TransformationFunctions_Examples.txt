Ex-1]map()

scala> val emp=sc.textFile("C:/Users/sashetye/demo/Employee.csv")
emp: org.apache.spark.rdd.RDD[String] = C:/Users/sashetye/demo/Employee.csv MapPartitionsRDD[58] at textFile at <console>:24

scala> val rows=emp.map(line => line.split(","))
rows: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[59] at map at <console>:25


or
import org.apache.spark.SparkContext
import org.apache.spark.SparkConf
import org.apache.spark.sql.SparkSession
object  mapTest{
def main(args: Array[String]) = {
val spark = SparkSession.builder.appName("mapExample").master("local").getOrCreate()
val data = spark.read.textFile("spark_test.txt").rdd
val mapFile = data.map(line => (line,line.length))
mapFile.foreach(println)
}
}
-----------------------------------------------------------------------------------------------------------
Ex-2]flatMap()

scala> val data=sc.parallelize(List(1,2,3)).flatMap(x =>List(x,x,x)).collect()
data: Array[Int] = Array(1, 1, 1, 2, 2, 2, 3, 3, 3)

scala> val data=sc.parallelize(List(1,2,3)).map(x =>List(x,x,x)).collect()
data: Array[List[Int]] = Array(List(1, 1, 1), List(2, 2, 2), List(3, 3, 3))

or

val data = spark.read.textFile("spark_test.txt").rdd
val flatmapFile = data.flatMap(lines => lines.split(" "))
flatmapFile.foreach(println)
-----------------------------------------------------------------------------------------------------------
Ex-3]filter()

"C:/Users/sashetye/demo/SparkEx-1.txt"

ERROR:You have error in your code

ERROR:Try agian to download MySQL

Please resolved the ERROR4

Now restart your MySQL database


scala> val myFile=sc.textFile("C:/Users/sashetye/demo/SparkEx-1.txt")

scala> val errors=myFile.filter(line => line.contains("ERROR")).foreach(println)

or

val data = spark.read.textFile("spark_test.txt").rdd
val mapFile = data.flatMap(lines => lines.split(" ")).filter(value => value=="spark")
println(mapFile.count())

-----------------------------------------------------------------------------------------------------------
Ex-4]mapPartitions()

scala> val partitions=sc.parallelize(1 to 9,3)
partitions: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[70] at parallelize at <console>:24

scala> partitions.mapPartitions(x =>List(x.next).iterator).collect()
res55: Array[Int] = Array(1, 4, 7)

scala> val partitions=sc.parallelize(1 to 9)
partitions: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[72] at parallelize at <console>:24

scala> partitions.mapPartitions(x =>List(x.next).iterator).collect()
res56: Array[Int] = Array(1, 2, 3, 4, 5, 6, 7, 8)
-----------------------------------------------------------------------------------------------------------
Ex-5]sample()
scala> val data=sc.parallelize(1 to 10)
data: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[74] at parallelize at <console>:24

scala> data.sample(true,.2).count()
res57: Long = 0

scala> data.sample(true,.1).foreach(println)
2
-----------------------------------------------------------------------------------------------------------
Ex-6]union()

scala> val p1=sc.parallelize(1 to 10)
p1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[80] at parallelize at <console>:24

scala> val p2=sc.parallelize(1 to 15)
p2: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[81] at parallelize at <console>:24

scala> p1.union(p2).collect()
res62: Array[Int] = Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15)

or

val rdd1 = spark.sparkContext.parallelize(Seq((1,"jan",2016),(3,"nov",2014),(16,"feb",2014)))
val rdd2 = spark.sparkContext.parallelize(Seq((5,"dec",2014),(17,"sep",2015)))
val rdd3 = spark.sparkContext.parallelize(Seq((6,"dec",2011),(16,"may",2015)))
val rddUnion = rdd1.union(rdd2).union(rdd3)
rddUnion.foreach(Println)

-----------------------------------------------------------------------------------------------------------
Ex-7]intersection()

scala> val p1=sc.parallelize(1 to 10)
p1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[80] at parallelize at <console>:24

scala> val p2=sc.parallelize(1 to 15)
p2: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[81] at parallelize at <console>:24

scala> p1.intersection(p2).collect()
res63: Array[Int] = Array(8, 1, 9, 10, 2, 3, 4, 5, 6, 7)

or

val rdd1 = spark.sparkContext.parallelize(Seq((1,"jan",2016),(3,"nov",2014, (16,"feb",2014)))
val rdd2 = spark.sparkContext.parallelize(Seq((5,"dec",2014),(1,"jan",2016)))
val comman = rdd1.intersection(rdd2)
comman.foreach(Println)
-----------------------------------------------------------------------------------------------------------
Ex-8]distinct()

scala> val p1=sc.parallelize(1 to 9)
p1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[89] at parallelize at <console>:24

scala> val p2=sc.parallelize(5 to 15)
p2: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[90] at parallelize at <console>:24

scala> p1.union(p2).distinct.collect()
res64: Array[Int] = Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15)

or

val rdd1 = park.sparkContext.parallelize(Seq((1,"jan",2016),(3,"nov",2014),(16,"feb",2014),(3,"nov",2014)))
val result = rdd1.distinct()
println(result.collect().mkString(", "))
-----------------------------------------------------------------------------------------------------------
Ex-9]grouByKey()

scala> val emp=sc.textFile("C:/Users/sashetye/demo/Employee.csv")
emp: org.apache.spark.rdd.RDD[String] = C:/Users/sashetye/demo/Employee.csv MapPartitionsRDD[1] at textFile at <console>:24

scala> val rows=emp.map(line =>line.split(","))
rows: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[2] at map at <console>:25

                 
scala> val names=rows.map(name =>(name(1),name(2)))
names: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[3] at map at <console>:25

scala> names.groupByKey.collect()
res0: Array[(String, Iterable[String])] = Array((Alex,CompactBuffer(Aus, Japan)), (Meena,CompactBuffer(America)), (Sam,CompactBuffer(Africa)), (FirstName,CompactBuffer(Country)), (John,CompactBuffer(Ind)))

scala> names.groupByKey.collect().foreach(println)
(Alex,CompactBuffer(Aus, Japan))
(Meena,CompactBuffer(America))
(Sam,CompactBuffer(Africa))
(FirstName,CompactBuffer(Country))
(John,CompactBuffer(Ind))

or

val data = spark.sparkContext.parallelize(Array(('k',5),('s',3),('s',4),('p',7),('p',5),('t',8),('k',6)),3)
val group = data.groupByKey().collect()
group.foreach(println)
-----------------------------------------------------------------------------------------------------------
Ex-10]reduceByKey()

scala> val emp=sc.textFile("C:/Users/sashetye/demo/Employee.csv")
emp: org.apache.spark.rdd.RDD[String] = C:/Users/sashetye/demo/Employee.csv MapPartitionsRDD[7] at textFile at <console>:24

scala> val rows=emp.filter(line => !line.contains("Alex")).map(line =>line.split(","))
rows: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[9] at map at <console>:25

scala> rows.map(n =>(n(1),n(2))).reduceByKey((v1,v2) =>v1+v2).collect.foreach(println)
(Meena,America)
(Sam,Africa)
(FirstName,Country)
(John,Ind)

or

val words = Array("one","two","two","four","five","six","six","eight","nine","ten")
val data = spark.sparkContext.parallelize(words).map(w => (w,1)).reduceByKey(_+_)
data.foreach(println)

-----------------------------------------------------------------------------------------------------------
Ex-11]sortByKey()

scala> val emp=sc.textFile("C:/Users/sashetye/demo/Employee.csv")
emp: org.apache.spark.rdd.RDD[String] = C:/Users/sashetye/demo/Employee.csv MapPartitionsRDD[36] at textFile at <console>:24

scala> val rows=emp.filter(line => !line.contains("Alex")).map(line =>line.split(","))
rows: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[38] at map at <console>:25

scala> rows.map(n =>(n(1),n(2))).sortByKey().foreach(println)
(FirstName,Country)
(Meena,America)
(Sam,Africa)
(John,Ind)

scala> rows.map(n =>(n(1),n(2))).sortByKey(false).foreach(println)
(Sam,Africa)
(Meena,America)
(John,Ind)
(FirstName,Country)

or

 val data = spark.sparkContext.parallelize(Seq(("maths",52), ("english",75), ("science",82), ("computer",65), ("maths",85)))
val sorted = data.sortByKey()
sorted.foreach(println)

o/p-
computer,65)
(english,75)
(maths,52)
(maths,85)
(science,82)
-----------------------------------------------------------------------------------------------------------
Ex-12]join()

scala> val names=sc.parallelize(List("abe","abby","apple")).map(a =>(a,1))
names: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[48] at map at <console>:24

scala> val names2=sc.parallelize(List("apple","beaty","beatrice")).map(a =>(a,1))
names2: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[50] at map at <console>:24

scala> names.join(names2).collect().foreach(println)
(apple,(1,1))             

scala> names.leftOuterJoin(names2).collect().foreach(println)
(abby,(1,None))
(apple,(1,Some(1)))
(abe,(1,None))

scala> names.rightOuterJoin(names2).collect().foreach(println)
(apple,(Some(1),1))
(beaty,(None,1))
(beatrice,(None,1))

or 

val data = spark.sparkContext.parallelize(Array(('A',1),('b',2),('c',3)))
val data2 =spark.sparkContext.parallelize(Array(('A',4),('A',6),('b',7),('c',3),('c',8)))
val result = data.join(data2)
println(result.collect().mkString(","))

-----------------------------------------------------------------------------------------------------------
Ex-13]coalesce()
val rdd1 = spark.sparkContext.parallelize(Array("jan","feb","mar","april","may","jun"),3)
val result = rdd1.coalesce(2)
result.foreach(println)

o/p-
mar
april
may 
jun
-----------------------------------------------------------------------------------------------------------
Ex-14]mapPartitions() and mapPartitionsWithIndex-

scala>  val inputData = sc.parallelize(1 to 9, 3)
inputData: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:24

scala>     val inputData1 = sc.parallelize(1 to 9)
inputData1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[1] at parallelize at <console>:24

scala>     val mapPartitionResult = inputData.mapPartitions(x => List(x.next).iterator)
mapPartitionResult: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[2] at mapPartitions at <console>:25

scala>     println("mapPartition is :" + mapPartitionResult.collect().mkString(","))
mapPartition is :1,4,7


scala>  val mapPartitionsWithIndexRseult = inputData.mapPartitionsWithIndex((index: Int, it: Iterator[Int])
     |     => it.toList.map(x => index + ", " + x).iterator)
mapPartitionsWithIndexRseult: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[3] at mapPartitionsWithIndex at <console>:25

scala>     println("mapPartitionsWithIndex :" + mapPartitionsWithIndexRseult.collect().mkString(","))
mapPartitionsWithIndex :0, 1,0, 2,0, 3,1, 4,1, 5,1, 6,2, 7,2, 8,2, 9


-----------------------------------------------------------------------------------------------------------
Ex-15]aggregateByKey-

scala> val names = sc.parallelize(List(("David", 6), ("Abby", 4), ("David", 5), ("Abby", 5)))
names: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[4] at parallelize at <console>:24

scala> names.reduceByKey((n,c) => n + c).collect
res2: Array[(String, Int)] = Array((Abby,9), (David,11))

scala> names.aggregateByKey(0)((k,v) => v.toInt+k, (v,k) => k+v).collect
res3: Array[(String, Int)] = Array((Abby,9), (David,11))
-----------------------------------------------------------------------------------------------------------

Ex-16]cogroup()-
scala>  val a = sc.parallelize(List(1, 2, 1, 3), 2)
a: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[8] at parallelize at <console>:24

scala>     val b = sc.parallelize(List(1, 2, 3, 4, 5, 6), 3)
b: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[9] at parallelize at <console>:24

scala>     val d = a.map((_, "b"))
d: org.apache.spark.rdd.RDD[(Int, String)] = MapPartitionsRDD[10] at map at <console>:25

scala> d.foreach(println)
(1,b)
(1,b)
(2,b)
(3,b)


scala>     val e = b.map((_, "c"))
e: org.apache.spark.rdd.RDD[(Int, String)] = MapPartitionsRDD[11] at map at <console>:25

scala> e.foreach(println)
(3,c)
(5,c)
(6,c)
(1,c)
(4,c)
(2,c)

scala>     val result = d.cogroup(e, 4)
result: org.apache.spark.rdd.RDD[(Int, (Iterable[String], Iterable[String]))] = MapPartitionsRDD[13] at cogroup at <console>:27

scala> result.foreach(println)
(4,(CompactBuffer(),CompactBuffer(c)))
(1,(CompactBuffer(b, b),CompactBuffer(c)))
(6,(CompactBuffer(),CompactBuffer(c)))
(3,(CompactBuffer(b),CompactBuffer(c)))
(2,(CompactBuffer(b),CompactBuffer(c)))
(5,(CompactBuffer(),CompactBuffer(c)))

-----------------------------------------------------------------------------------------------------------
Ex-17]repartitionAndSortWithinPartitions()-

scala> val randRDD = sc.parallelize(List( (2,"cat"), (6, "mouse"),(7, "cup"), (3, "book"), (4, "tv"), (1, "screen"), (5, "heater")), 3)
randRDD: org.apache.spark.rdd.RDD[(Int, String)] = ParallelCollectionRDD[16] at parallelize at <console>:24

scala> val rPartitioner = new org.apache.spark.RangePartitioner(3, randRDD)
rPartitioner: org.apache.spark.RangePartitioner[Int,String] = org.apache.spark.RangePartitioner@850c

scala> val partitioned = randRDD.partitionBy(rPartitioner)
partitioned: org.apache.spark.rdd.RDD[(Int, String)] = ShuffledRDD[19] at partitionBy at <console>:27

scala> def myfunc(index: Int, iter: Iterator[(Int, String)]) : Iterator[String] = {
     |   iter.map(x => "[partID:" +  index + ", val: " + x + "]")
     | }
myfunc: (index: Int, iter: Iterator[(Int, String)])Iterator[String]

scala> partitioned.mapPartitionsWithIndex(myfunc).collect
res15: Array[String] = Array([partID:0, val: (2,cat)], [partID:0, val: (3,book)], [partID:0, val: (1,screen)], [partID:1, val: (4,tv)], [partID:1, val: (5,heater)], [partID:2, val: (6,mouse)], [partID:2, val: (7,cup)])

scala> val partitioned = randRDD.repartitionAndSortWithinPartitions(rPartitioner)
partitioned: org.apache.spark.rdd.RDD[(Int, String)] = ShuffledRDD[21] at repartitionAndSortWithinPartitions at <console>:27

scala> def myfunc(index: Int, iter: Iterator[(Int, String)]) : Iterator[String] = {
     |   iter.map(x => "[partID:" +  index + ", val: " + x + "]")
     | }
myfunc: (index: Int, iter: Iterator[(Int, String)])Iterator[String]

scala> partitioned.mapPartitionsWithIndex(myfunc).collect
res16: Array[String] = Array([partID:0, val: (1,screen)], [partID:0, val: (2,cat)], [partID:0, val: (3,book)], [partID:1, val: (4,tv)], [partID:1, val: (5,heater)], [partID:2, val: (6,mouse)], [partID:2, val: (7,cup)])
-----------------------------------------------------------------------------------------------------------
Ex-18]saveAsObjectFile()-
val x = sc.parallelize(1 to 100, 3)
x.saveAsObjectFile("objFile")
val y = sc.objectFile[Int]("objFile")
y.collect
-----------------------------------------------------------------------------------------------------------

ex-19]takeOrdered()-

scala> val b = sc.parallelize(List("dog", "cat", "ape", "salmon", "gnu"), 2)
b: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[44] at parallelize at <console>:24

scala> b.takeOrdered(2)
res29: Array[String] = Array(ape, cat)
